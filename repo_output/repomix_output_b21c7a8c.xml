This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-10-15 22:58:39

# File Summary

## Purpose:

This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format:

The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
   a. A header with the file path (## File: path/to/file)
   b. The full contents of the file in a code block

## Usage Guidelines:

- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes:

- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

## Additional Information:

For more information about Repomix, visit: https://github.com/andersonby/python-repomix


# Repository Structure

```
.gitignore
app
  api
    project_routes.py
    user_routes.py
  db
    connect_db.py
  main.py
  schemas
    memory_schemas.py
  services
    cache_service.py
    project_service.py
    save_memory_service.py
  vector_db
    embed.py
docs
  deployment.md
  README.md
migrations
  enable_pgvector.sql
  env.py
  README
  script.py.mako
  versions
    618c51757f22_add_missing_fields_to_memory_model.py
    694a99143351_revert_memory_model_to_match_original_.py
    add_pgvector_support.py
    add_project_fields_20251013.py
    add_supabase_user_id_20251009.py
    drop_projects_name_unique_20251013.py
    merge_20251013_02.py
    merge_9f1b2c3d4e5f_add_pgvector_support.py
    merge_addsupabase_and_merge9f_20251013.py
README.md
tests
  test.ipynb
```

# Repository Files


## .gitignore

```text
.env
*__pycache__/
*.pyc
*__init__.py
.vscode/
```

## app/api/project_routes.py

```python
from fastapi import APIRouter, Depends, HTTPException, status, Header
from sqlalchemy.ext.asyncio import AsyncSession
import logging
from typing import List, Optional
from uuid import UUID
from fastapi import Path

from app.schemas.memory_schemas import CreateProjectRequest, ProjectResponse, UpdateProjectRequest
from app.services.project_service import ProjectService
from app.db.connect_db import get_db_session

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/v1/projects", tags=["projects"])


async def get_project_service(
    db: AsyncSession = Depends(get_db_session)
) -> ProjectService:
    return ProjectService(db=db)


@router.post("/create", response_model=ProjectResponse, status_code=status.HTTP_201_CREATED)
async def create_project(
    request: CreateProjectRequest,
    user_id: Optional[str] = Header(None, alias="X-User-ID"),
    project_service: ProjectService = Depends(get_project_service)
):
    try:
        if not user_id:
            user_id = "12345678-1234-5678-9012-123456789012"
        
        user_uuid = UUID(user_id)
        
        project = await project_service.create_project(
            request=request,
            user_id=user_uuid  
        )
        
        project_response = ProjectResponse(
            id=project.id,
            name=project.name,
            description=project.description,
            is_active=project.is_active,
            repo_url=project.repo_url,
            technologies=project.technologies,
            memories_count=project.memories_count,
            members_count=project.members_count,
            last_active_at=project.last_active_at,
            settings=project.settings,
            created_at=project.created_at,
            updated_at=project.updated_at
        )
        
        logger.info(f"Project created successfully: {project.id}")
        return project_response
        
    except ValueError as e:
        logger.error(f"Validation error creating project: {e}")
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail=str(e)
        )
    except Exception as e:
        logger.error(f"Unexpected error creating project: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to create project"
        )

@router.patch("/{project_id}", response_model=ProjectResponse)
async def edit_project(
    # project_id: UUID,
    request: UpdateProjectRequest,
    user_id: Optional[str] = Header(None, alias="X-User-ID"),
    project_service: ProjectService = Depends(get_project_service),
    project_id: UUID = Path(...),
):
    try:
        if not user_id:
            user_id = "12345678-1234-5678-9012-123456789012"

        user_uuid = UUID(user_id)
        
        project = await project_service.update_project(
            project_id=project_id,
            request=request,
            user_id=user_uuid
        )
        
        project_response = ProjectResponse(
            id=project.id,
            name=project.name,
            description=project.description,
            is_active=project.is_active,
            repo_url=project.repo_url,
            technologies=project.technologies,
            memories_count=project.memories_count,
            members_count=project.members_count,
            last_active_at=project.last_active_at,
            settings=project.settings,
            created_at=project.created_at,
            updated_at=project.updated_at
        )
        
        logger.info(f"Project edited successfully: {project.id}")
        return project_response
        
    except ValueError as e:
        logger.error(f"Validation error editing project: {e}")
        raise HTTPException(
            status_code=status.HTTP_409_CONFLICT,
            detail=str(e)
        )
    except Exception as e:
        logger.error(f"Unexpected error editing project: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to edit project"
        )
    


@router.get("/projects/{project_id}/recent", response_model=list)
async def get_recent_memories(
    project_id: UUID,
    limit: int = 10,
    days: int = 7,
    user_id: Optional[str] = Header(None, alias="X-User-ID"),
    project_service: ProjectService = Depends(get_project_service)
):
    try:
        user_uuid = None
        if user_id:
            try:
                user_uuid = UUID(user_id)
            except ValueError:
                pass
        
        memories = await project_service.get_recent_memories(
            project_id=project_id,
            user_id=user_uuid,
            limit=min(limit, 50),
            days=min(days, 30)
        )
        
        memory_list = []
        for memory in memories:
            memory_dict = {
                "id": str(memory.id),
                "content": memory.content,
                "summary": memory.summary,
                "tags": memory.tags or [],
                "project_id": str(memory.project_id),
                "created_at": memory.created_at.isoformat(),
                "updated_at": memory.updated_at.isoformat() if memory.updated_at else None
            }
            memory_list.append(memory_dict)
        
        logger.info(f"Retrieved {len(memory_list)} recent memories for project {project_id}")
        return memory_list
        
    except ValueError as e:
        logger.error(f"Get recent memories error: {e}")
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=str(e)
        )
    except Exception as e:
        logger.error(f"Unexpected error getting recent memories: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to retrieve recent memories"
        )


@router.get("/user/projects", response_model=list)
async def get_user_projects(
    user_id: Optional[str] = Header(None, alias="X-User-ID"),
    project_service: ProjectService = Depends(get_project_service)
):
    try:
        if not user_id:
            user_id = "12345678-1234-5678-9012-123456789012"
        
        user_uuid = UUID(user_id)
        
        projects = await project_service.get_user_projects(user_uuid)
        
        project_list = []
        for project in projects:
            project_dict = {
                "id": str(project.id),
                "name": project.name,
                "description": project.description,
                "is_active": project.is_active,
                "repo_url": project.repo_url,
                "technologies": project.technologies,
                "memories_count": project.memories_count,
                "members_count": project.members_count,
                "last_active_at": project.last_active_at.isoformat() if project.last_active_at else None,
                "settings": project.settings,
                "created_at": project.created_at.isoformat(),
                "updated_at": project.updated_at.isoformat() if project.updated_at else None
            }
            project_list.append(project_dict)
        
        logger.info(f"Retrieved {len(project_list)} projects for user {user_uuid}")
        return project_list
        
    except Exception as e:
        logger.error(f"Unexpected error getting user projects: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to retrieve user projects"
        )


@router.get("/projects/{project_id}", response_model=ProjectResponse)
async def get_project_details(
    project_id: UUID,
    user_id: Optional[str] = Header(None, alias="X-User-ID"),
    project_service: ProjectService = Depends(get_project_service)
):
    try:
        if not user_id:
            user_id = "12345678-1234-5678-9012-123456789012"
        
        user_uuid = UUID(user_id)
        
        project = await project_service.get_project_by_id(project_id, user_uuid)
        
        if not project:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Project not found or access denied"
            )
        
        project_response = ProjectResponse(
            id=project.id,
            name=project.name,
            description=project.description,
            is_active=project.is_active,
            repo_url=project.repo_url,
            technologies=project.technologies,
            memories_count=project.memories_count,
            members_count=project.members_count,
            last_active_at=project.last_active_at,
            settings=project.settings,
            created_at=project.created_at,
            updated_at=project.updated_at
        )
        
        logger.info(f"Retrieved project details for {project_id}")
        return project_response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Unexpected error getting project details: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to retrieve project details"
        )
```

## app/api/user_routes.py

```python
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession
from uuid import UUID, uuid4
import logging

from app.db.connect_db import get_db_session
from app.models.memory_models import User
from pydantic import BaseModel, EmailStr
from sqlalchemy.exc import IntegrityError
import traceback


from app.services.supabase_admin import create_supabase_user

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/v1/users", tags=["users"])


class CreateUserRequest(BaseModel):
    email: EmailStr
    name: str | None = None


class CreateUserResponse(BaseModel):
    id: UUID
    email: EmailStr
    name: str | None = None


@router.post("/", response_model=CreateUserResponse, status_code=status.HTTP_201_CREATED)
async def create_user(
    req: CreateUserRequest,
    db: AsyncSession = Depends(get_db_session)
):
    try:
        user_id = uuid4()

        try:
            supabase_uid = await create_supabase_user(req.email, name=req.name)
        except Exception as e:
            logger.warning(f"Supabase admin create failed: {e}")
            supabase_uid = None

        # create local user
        new_user = User(id=user_id, email=req.email, name=req.name, supabase_user_id=supabase_uid)
        db.add(new_user)
        await db.flush()

        logger.info(f"Created user {new_user.id}")
        return CreateUserResponse(id=new_user.id, email=new_user.email, name=new_user.name)
    except Exception as e:
        logger.error(f"Failed to create user: {e}")
        raise HTTPException(status_code=500, detail="Failed to create user")
```

## app/db/connect_db.py

```python
import asyncio
import sys
from pathlib import Path
import logging
from typing import AsyncGenerator, Optional

import redis
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker, declarative_base
from sqlalchemy import text, event

from config.config import DATABASE_URL, REDIS_URL

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
redis_logger = logging.getLogger('redis')
redis_logger.setLevel(logging.DEBUG)

Base = declarative_base()

class DatabaseManager:
    def __init__(self):
        self.async_pg_engine = None
        self.async_session_factory = None
        self.redis_client = None

    # PostgreSQL
    async def initialize_postgresql(self) -> bool:
        try:
            self.async_pg_engine = create_async_engine(
                DATABASE_URL.replace("postgresql://", "postgresql+asyncpg://"),
                echo=False,
                pool_pre_ping=True,
                pool_recycle=3600
            )

            self.async_session_factory = sessionmaker(
                bind=self.async_pg_engine,
                class_=AsyncSession,
                expire_on_commit=False
            )

            # Test connection
            async with self.async_pg_engine.begin() as conn:
                await conn.execute(text("SELECT 1"))
            
            self._register_pgvector()
            logger.info("PostgreSQL connection initialized successfully")
            return True

        except Exception as e:
            logger.error(f"Failed to initialize PostgreSQL: {e}")
            return False

    def _register_pgvector(self):
        try:
            def _register(dbapi_conn, _):
                try:
                    from pgvector.asyncpg import register_vector as _reg
                    maybe = _reg(dbapi_conn)
                    if asyncio.iscoroutine(maybe):
                        loop = asyncio.get_running_loop()
                        task = loop.create_task(maybe)
                        task.add_done_callback(
                            lambda fut: logger.debug(f"pgvector registration error: {fut.exception()}")
                            if fut.exception() else None
                        )
                    logger.info("Registered pgvector codec")
                except Exception as e:
                    logger.debug(f"pgvector registration skipped: {e}")

            event.listen(self.async_pg_engine.sync_engine, "connect", _register)
        except Exception as e:
            logger.warning(f"Failed to attach pgvector listener: {e}")

    async def get_async_session(self) -> AsyncSession:
        if not self.async_session_factory:
            await self.initialize_postgresql()
        return self.async_session_factory()

    # Redis
    def initialize_redis(self) -> bool:
        max_retries = 3
        for attempt in range(max_retries):
            try:
                if not REDIS_URL:
                    logger.warning("REDIS_URL not configured")
                    return False

                logger.info(f"Redis connection attempt {attempt + 1}/{max_retries}")
                self.redis_client = redis.from_url(
                    REDIS_URL,
                    decode_responses=True,
                    socket_keepalive=True,
                    socket_connect_timeout=10,
                    socket_timeout=5,
                    retry_on_timeout=True,
                    health_check_interval=30
                )

                # Test Redis
                if self.redis_client.ping():
                    test_key = "connection_test"
                    self.redis_client.setex(test_key, 10, "test_value")
                    test_value = self.redis_client.get(test_key)
                    logger.info(f"Redis test operation: {test_value}")
                    logger.info("Redis connection initialized successfully")
                    return True

            except redis.ConnectionError as e:
                logger.warning(f"Redis connection attempt {attempt + 1} failed: {e}")
            except Exception as e:
                logger.error(f"Redis initialization error: {e}")

        logger.error("All Redis connection attempts failed")
        return False

    def get_redis_client(self) -> redis.Redis:
        if not self.redis_client:
            self.initialize_redis()
        return self.redis_client

    async def close_connections(self):
        try:
            if self.async_pg_engine:
                await self.async_pg_engine.dispose()
                logger.info("PostgreSQL connections closed")
            if self.redis_client:
                self.redis_client.close()
                logger.info("Redis connections closed")
        except Exception as e:
            logger.error(f"Error closing connections: {e}")

# -----------------------------
# Globals and helpers
# -----------------------------
db_manager = DatabaseManager()

async def get_db_session() -> AsyncGenerator[AsyncSession, None]:
    session = await db_manager.get_async_session()
    try:
        yield session
        await session.commit()
    except Exception:
        await session.rollback()
        raise
    finally:
        await session.close()

def get_redis() -> Optional[redis.Redis]:
    try:
        return db_manager.get_redis_client()
    except Exception as e:
        logger.warning(f"Redis not available: {e}")
        return None

async def initialize_all_databases() -> bool:
    pg_success = await db_manager.initialize_postgresql()
    redis_success = db_manager.initialize_redis()
    if pg_success and redis_success:
        logger.info("All databases initialized successfully!")
        return True
    else:
        logger.error("Some database connections failed")
        return False

if __name__ == "__main__":
    async def main():
        success = await initialize_all_databases()
        await db_manager.close_connections()
        return success
```

## app/main.py

```python
import logging
import sys
import os
from contextlib import asynccontextmanager
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from app.db.connect_db import db_manager, initialize_all_databases
from app.api.memory_routes import router as memory_router
from app.api.project_routes import router as project_router
from app.api.apikey_routes import router as apikey_router
from app.api.user_routes import router as user_router
from app.api.supabase_webhooks import router as supabase_webhook_router

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager."""
    # Startup
    logger.info("Starting CodeMemory Backend...")
    
    try:
        # Initialize database connections
        success = await initialize_all_databases()
        if success:
            logger.info("Database connections initialized")
        else:
            logger.error("Failed to initialize databases")
    except Exception as e:
        logger.error(f"Startup error: {e}")

    yield
    
    # Shutdown
    logger.info("Shutting down CodeMemory Backend...")
    try:
        await db_manager.close_connections()
        logger.info("Database connections closed")
    except Exception as e:
        logger.error(f"Shutdown error: {e}")


app = FastAPI(
    title="CodeMemory Backend",
    description="An intelligent memory system for AI coding agents",
    version="1.0.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(memory_router)
app.include_router(project_router)
app.include_router(apikey_router)
app.include_router(user_router)
app.include_router(supabase_webhook_router)



@app.get("/")
async def root():
    return {"message": "CodeMemory Backend API", "version": "1.0.0"}


@app.get("/health")
async def health_check():
    try:
        health_status = await db_manager.test_connections()
        
        return {
            "status": "healthy" if health_status else "unhealthy",
            "services": {
                "postgresql": "up" if health_status else "down",
                "redis": "up" if health_status else "down"
            }
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
```

## app/schemas/memory_schemas.py

```python
from datetime import datetime
from typing import Any, Dict, List, Optional
from uuid import UUID

from pydantic import BaseModel, Field, validator


class SaveMemoryRequest(BaseModel):
    content: str = Field(..., min_length=1, max_length=50000)
    project_id: UUID
    tags: Optional[List[str]] = Field(default=None, max_items=20)
    metadata: Optional[Dict[str, Any]] = Field(default=None)

    @validator('content')
    def validate_content(cls, v):
        if not v or not v.strip():
            raise ValueError('Content cannot be empty')
        return v.strip()

    @validator('tags')
    def validate_tags(cls, v):
        if v is not None:
            return list(set([tag.strip() for tag in v if tag.strip()]))[:20]
        return v


class MemoryResponse(BaseModel):
    id: UUID
    content: str
    tags: List[str]
    created_at: datetime
    updated_at: datetime
    project_id: UUID
    meta_data: Dict[str, Any]
    usage_count: int = 0
    embedding_dimensions: Optional[int] = None

    class Config:
        from_attributes = True


class SearchMemoryRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=1000)
    project_id: Optional[UUID] = None
    tags: Optional[List[str]] = None
    limit: int = Field(default=10, ge=1, le=100)
    similarity_threshold: float = Field(default=0.7, ge=0.0, le=1.0)
    top_k: int = Field(default=10, ge=1, le=50)


class SearchHit(BaseModel):
    id: UUID
    content: str
    summary: Optional[str] = None
    tags: List[str] = []
    project_id: UUID
    created_at: datetime
    score: float
    search_type: str
    rank: Optional[int] = None


class SearchResultsResponse(BaseModel):
    results: List[SearchHit]
    count: int

    class Config:
        from_attributes = True


class GetMemoriesRequest(BaseModel):
    project_id: UUID
    page: int = Field(default=1, ge=1)
    limit: int = Field(default=20, ge=1, le=100)
    tags: Optional[List[str]] = None
    search_content: Optional[str] = None


class GetMemoriesResponse(BaseModel):
    memories: List[MemoryResponse]
    total: int
    page: int
    limit: int
    total_pages: int


# ============= PROJECT SCHEMAS =============
class CreateProjectRequest(BaseModel):
    name: str = Field(..., min_length=1, max_length=100)

    description: Optional[str] = Field(default=None, max_length=1000)
    is_active: Optional[bool] = Field(default=True)
    repo_url: Optional[str] = Field(default=None, max_length=1024)
    technologies: Optional[List[str]] = Field(default=None)
    settings: Optional[Dict[str, Any]] = Field(default=None)

    @validator('name')
    def validate_name(cls, v):
        if not v or not v.strip():
            raise ValueError('Project name cannot be empty')
        return v.strip()

    @validator('description')
    def validate_description(cls, v):
        if v is not None:
            return v.strip() if v.strip() else None
        return v


class UpdateProjectRequest(BaseModel):
    name: Optional[str] = Field(default=None, min_length=1, max_length=100)
    description: Optional[str] = Field(default=None, max_length=1000)
    is_active: Optional[bool] = Field(default=None)
    repo_url: Optional[str] = Field(default=None, max_length=1024)
    technologies: Optional[List[str]] = Field(default=None)
    settings: Optional[Dict[str, Any]] = Field(default=None)

    @validator('name')
    def validate_name(cls, v):
        if v is not None:
            if not v or not v.strip():
                raise ValueError('Project name cannot be empty')
            return v.strip()
        return v

    @validator('description')
    def validate_description_optional(cls, v):
        if v is not None:
            return v.strip() if v.strip() else None
        return v

class ProjectResponse(BaseModel):
    id: UUID
    name: str
    description: Optional[str]
    is_active: bool
    repo_url: Optional[str]
    technologies: Optional[List[str]]
    memories_count: int = 0
    members_count: int = 0
    last_active_at: Optional[datetime]
    settings: Optional[Dict[str, Any]]
    created_at: datetime
    updated_at: Optional[datetime]

    class Config:
        from_attributes = True


class GetRecentMemoriesRequest(BaseModel):
    project_id: UUID
    limit: int = Field(default=10, ge=1, le=50)
    days: int = Field(default=7, ge=1, le=30)


class GetRecentMemoriesResponse(BaseModel):
    memories: List[MemoryResponse]
    total: int

    class Config:
        from_attributes = True
```

## app/services/cache_service.py

```python

```

## app/services/project_service.py

```python
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, desc
from sqlalchemy.exc import IntegrityError
from typing import Optional, List
from datetime import datetime, timedelta
from uuid import UUID
import logging

from app.models.memory_models import Project, UserProject, Memory, User
from app.schemas.memory_schemas import CreateProjectRequest

logger = logging.getLogger(__name__)


class ProjectService:
    def __init__(self, db: AsyncSession):
        self.db = db

    # Create project and assign owner
    async def create_project(self, request: CreateProjectRequest, user_id: UUID) -> Project:
        try:
            user = await self._ensure_user_exists(user_id)
            repo_url = getattr(request, 'repo_url', None)
            if repo_url:
                result = await self.db.execute(select(Project).where(Project.repo_url == repo_url))
                existing = result.scalar_one_or_none()
                if existing:
                    raise ValueError(f"Project with repo_url '{repo_url}' already exists")

            # Create project
            project = Project(
                name=request.name,
                description=request.description,
                is_active=getattr(request, 'is_active', True),
                repo_url=repo_url,
                technologies=getattr(request, 'technologies', None),
                settings=request.settings or {},
                members_count=1
            )
            self.db.add(project)
            await self.db.flush()

            # Assign user as owner
            self.db.add(UserProject(user_id=user_id, project_id=project.id, role="owner"))
            await self.db.commit()
            await self.db.refresh(project)

            logger.info(f"Created project {project.id} for user {user_id}")
            return project

        except ValueError:
            await self.db.rollback()
            raise
        except IntegrityError as e:
            # Handle DB unique constraint violations and map to ValueError so API layer
            # can return a 409 Conflict instead of 500.
            await self.db.rollback()
            msg = str(e)
            # Try to inspect underlying DB error message if available
            try:
                orig = e.orig
                msg = str(orig)
            except Exception:
                pass

            # Postgres typical message contains the constraint name, e.g.
            # 'duplicate key value violates unique constraint "projects_name_key"'
            if 'projects_name_key' in msg or 'projects_name' in msg or 'duplicate key value violates unique constraint' in msg and 'name' in msg:
                raise ValueError(f"Project with name '{request.name}' already exists")
            if 'projects_repo_url_key' in msg or 'repo_url' in msg:
                raise ValueError(f"Project with repo_url '{repo_url}' already exists")

            # Fallback
            logger.error(f"Integrity error creating project: {msg}")
            raise
        except Exception as e:
            await self.db.rollback()
            logger.error(f"Failed to create project: {e}")
            raise
        
    async def _ensure_user_exists(self, user_id: UUID) -> User:
        result = await self.db.execute(select(User).where(User.id == user_id))
        user = result.scalar_one_or_none()
        if not user:
            user = User(
                id=user_id,
                email=f"user-{user_id}@test.com",
                name="Test User"
            )
            self.db.add(user)
            await self.db.flush()
            logger.info(f"Created test user {user_id}")
        return user

    # Retrieve user projects
    async def get_user_projects(self, user_id: UUID) -> List[Project]:
        try:
            result = await self.db.execute(
                select(Project)
                .join(UserProject)
                .where(UserProject.user_id == user_id)
                .order_by(desc(Project.created_at))
            )
            return result.scalars().all()
        except Exception as e:
            logger.error(f"Failed to get user projects: {e}")
            raise Exception("Failed to retrieve projects")

    # Retrieve project by id with user check
    async def get_project_by_id(self, project_id: UUID, user_id: UUID) -> Optional[Project]:
        try:
            result = await self.db.execute(
                select(Project)
                .join(UserProject)
                .where(Project.id == project_id, UserProject.user_id == user_id)
            )
            return result.scalar_one_or_none()
        except Exception as e:
            logger.error(f"Failed to get project {project_id}: {e}")
            return None

    # Get recent memories for a project
    async def get_recent_memories(
        self,
        project_id: UUID,
        user_id: Optional[UUID] = None,
        limit: int = 10,
        days: int = 7
    ) -> List[Memory]:
        try:
            project = await self._get_project_with_access(project_id, user_id)

            date_threshold = datetime.utcnow() - timedelta(days=days)
            result = await self.db.execute(
                select(Memory)
                .where(Memory.project_id == project_id, Memory.created_at >= date_threshold)
                .order_by(desc(Memory.created_at))
                .limit(limit)
            )
            memories = result.scalars().all()
            logger.info(f"Retrieved {len(memories)} recent memories for project {project_id}")
            return memories

        except ValueError:
            raise
        except Exception as e:
            logger.error(f"Failed to get recent memories: {e}")
            raise Exception("Failed to retrieve recent memories")

    async def _get_project_with_access(self, project_id: UUID, user_id: Optional[UUID]) -> Project:
        if user_id:
            project = await self.get_project_by_id(project_id, user_id)
            if not project:
                raise ValueError("Project not found or access denied")
        else:
            result = await self.db.execute(select(Project).where(Project.id == project_id))
            project = result.scalar_one_or_none()
            if not project:
                raise ValueError("Project not found")
        return project
    
    # edit project
    async def edit_project(self, project_id: UUID, user_id: UUID, request: CreateProjectRequest) -> Project:
        try:
            project = await self._get_project_by_id(project_id, user_id)
            project.name = request.name
            project.description = request.description
            project.is_active = request.is_active
            project.repo_url = str(request.repo_url) if request.repo_url else None
            project.settings = request.settings or {}
            project.technologies = request.technologies
            project.updated_at = datetime.utcnow()
            await self.db.commit()
            await self.db.refresh(project)
            return project
        except ValueError:
            raise ValueError("Project not found")

    # Update project
    async def update_project(self, project_id: UUID, request, user_id: UUID) -> Project:
        try:
            project = await self._get_project_with_access(project_id, user_id)

            if getattr(request, 'name', None) is not None:
                project.name = request.name
            if getattr(request, 'description', None) is not None:
                project.description = request.description
            if getattr(request, 'is_active', None) is not None:
                project.is_active = request.is_active
            if getattr(request, 'repo_url', None) is not None:
                repo_url = str(request.repo_url) if request.repo_url else None
                if repo_url:
                    res = await self.db.execute(select(Project).where(Project.repo_url == repo_url, Project.id != project_id))
                    existing = res.scalar_one_or_none()
                    if existing:
                        raise ValueError(f"Project with repo_url '{repo_url}' already exists")
                project.repo_url = repo_url
            if getattr(request, 'technologies', None) is not None:
                project.technologies = request.technologies
            if getattr(request, 'settings', None) is not None:
                project.settings = request.settings or {}

            project.updated_at = datetime.utcnow()
            await self.db.commit()
            await self.db.refresh(project)
            return project
        except IntegrityError as e:
            await self.db.rollback()
            msg = str(e)
            try:
                orig = e.orig
                msg = str(orig)
            except Exception:
                pass

            if 'projects_repo_url_key' in msg or 'repo_url' in msg:
                raise ValueError(f"Project with repo_url '{getattr(request, 'repo_url', None)}' already exists")
            if 'projects_name_key' in msg or 'name' in msg:
                raise ValueError("Project with this name already exists")
            logger.error(f"Integrity error updating project: {msg}")
            raise
        except ValueError:
            raise
        except Exception as e:
            await self.db.rollback()
            logger.error(f"Failed to update project: {e}")
            raise
```

## app/services/save_memory_service.py

```python
import hashlib
import json
import logging
import sys
import os
from datetime import datetime
from typing import Dict, List, Optional
from uuid import UUID, uuid4

project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
from redis import Redis

from app.models.memory_models import Memory, UserProject
from app.vector_db.embed import get_embedding_model

logger = logging.getLogger(__name__)


class SaveMemoryService:
    def __init__(self, db: AsyncSession, redis: Optional[Redis] = None):
        self.db = db
        self.redis = redis
        self.embedding_model = get_embedding_model()

    async def save_memory(
        self,
        content: str,
        project_id: UUID,
        user_id: UUID,
        tags: List[str] = None,
        metadata: Dict = None
    ) -> Memory:
        try:
            await self._validate_input(content, project_id, user_id)
            processed_content = self._sanitize_content(content)
            embedding_vector = await self._generate_embedding(processed_content)
            processed_tags = tags or await self._auto_generate_tags(processed_content)
            
            # Save to database (following ERD schema)
            memory = await self._save_to_database(
                content=processed_content,
                embedding=embedding_vector,
                project_id=project_id,
                tags=processed_tags,
                metadata=metadata
            )
            
            await self._cache_memory(memory, user_id, project_id)
            
            logger.info(f"Memory saved successfully: {memory.id}")
            return memory
            
        except Exception as e:
            logger.error(f"Failed to save memory: {str(e)}")
            raise

    async def _validate_input(self, content: str, project_id: UUID, user_id: UUID):
        if not content or not content.strip():
            raise ValueError("Content cannot be empty")
        
        if len(content) > 50000:
            raise ValueError("Content too large (max 50KB)")
        
    def _sanitize_content(self, content: str) -> str:
        return ' '.join(content.split()).strip()

    async def _generate_embedding(self, content: str) -> Optional[List[float]]:
        try:
            embedding = await self.embedding_model.aembed_query(content)
            return embedding
        except Exception as e:
            logger.warning(f"Failed to generate embedding: {str(e)}")
            return None

    async def _auto_generate_tags(self, content: str) -> List[str]:
        words = content.lower().split()
        
        programming_keywords = {
            'python', 'javascript', 'react', 'fastapi', 'sql', 'database',
            'api', 'authentication', 'security', 'async', 'function', 'class'
        }
        
        tags = []
        for word in words:
            clean_word = ''.join(char for char in word if char.isalnum())
            if clean_word in programming_keywords and clean_word not in tags:
                tags.append(clean_word)
                
        return tags[:10]

    async def _save_to_database(
        self,
        content: str,
        embedding: Optional[List[float]],
        project_id: UUID,
        tags: List[str],
        metadata: Optional[Dict]
    ) -> Memory:
        memory = Memory(
            id=uuid4(),
            project_id=project_id,
            content=content,
            embedding=embedding,
            tags=tags or [],
            meta_data=metadata or {},
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow()
        )
        
        self.db.add(memory)
        await self.db.flush()
        
        return memory

    async def _cache_memory(self, memory: Memory, user_id: UUID, project_id: UUID):
        try:
            if self.redis: 
                cache_key = f"memory:{memory.id}"
                memory_data = {
                    "id": str(memory.id),
                    "content": memory.content,
                    "tags": memory.tags,
                    "created_at": memory.created_at.isoformat(),
                    "project_id": str(memory.project_id)
                }
                
                self.redis.setex(cache_key, 3600, json.dumps(memory_data))
                
                user_recent_key = f"user:{user_id}:recent_memories"
                self.redis.lpush(user_recent_key, str(memory.id))
                self.redis.ltrim(user_recent_key, 0, 49)
                self.redis.expire(user_recent_key, 86400)
                
                project_cache_key = f"project:{project_id}:memories"
                self.redis.delete(project_cache_key)
            else:
                logger.info("Redis not available, skipping cache operations")
            
        except Exception as e:
            logger.warning(f"Failed to cache memory: {str(e)}")
```

## app/vector_db/embed.py

```python
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import os
import sys
import dotenv
dotenv.load_dotenv()

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))
from config.config import GOOGLE_API_KEY, EMBEDDING_MODEL_NAME

embedding_model = GoogleGenerativeAIEmbeddings(
    model=EMBEDDING_MODEL_NAME, 
    api_key=GOOGLE_API_KEY
)

def get_embedding_model():
    return embedding_model

if __name__ == "__main__":
    model = get_embedding_model()
    text = "Hello, world!"
    embedding = model.embed_query(text)
    print(f"Embedding for '{text}': {embedding}")
    print(f"Embedding dimension: {len(embedding)}")
```

## docs/deployment.md

````markdown
# Deployment Architecture

## Multi-User Production Setup

### ðŸ—„ï¸ **Database Layer**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Supabase           â”‚    â”‚   Redis Cloud   â”‚
â”‚    (PostgreSQL)         â”‚    â”‚   (Caching)     â”‚
â”‚                         â”‚    â”‚                 â”‚
â”‚ â€¢ User data             â”‚    â”‚ â€¢ Query cache   â”‚
â”‚ â€¢ Metadata              â”‚    â”‚ â€¢ Sessions      â”‚
â”‚ â€¢ Projects              â”‚    â”‚ â€¢ Rate limits   â”‚
â”‚ â€¢ Vector embeddings     â”‚    â”‚                 â”‚
â”‚ â€¢ Similarity search     â”‚    â”‚                 â”‚
â”‚   (pgvector extension)  â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ðŸš€ **Application Layer**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Backend                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚   Memory    â”‚ â”‚   Search    â”‚ â”‚      Analytics          â”‚â”‚
â”‚  â”‚  Service    â”‚ â”‚  Service    â”‚ â”‚      Service            â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
````

## docs/README.md

```markdown
# API Documentation

This directory contains API documentation files.

## Contents
- OpenAPI/Swagger documentation
- API design documents
- Integration guides
```

## migrations/enable_pgvector.sql

```sql
-- Enable pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Test pgvector installation
SELECT * FROM pg_extension WHERE extname = 'vector';
```

## migrations/env.py

```python
from logging.config import fileConfig
import sys
import os
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from sqlalchemy import engine_from_config
from sqlalchemy import pool

from alembic import context

# Import your models
from app.models.memory_models import Base
from config.config import DATABASE_URL

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Set the database URL from environment
config.set_main_option('sqlalchemy.url', DATABASE_URL)

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
target_metadata = Base.metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    connectable = engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()

if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
```

## migrations/README

```text
Generic single-database configuration.
```

## migrations/script.py.mako

```text
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, Sequence[str], None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    """Upgrade schema."""
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    """Downgrade schema."""
    ${downgrades if downgrades else "pass"}
```

## migrations/versions/618c51757f22_add_missing_fields_to_memory_model.py

```python
"""Add missing fields to Memory model

Revision ID: 618c51757f22
Revises: e3704b220fd3
Create Date: 2025-10-01 18:21:31.539030

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '618c51757f22'
down_revision: Union[str, Sequence[str], None] = 'e3704b220fd3'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('memories', sa.Column('user_id', sa.UUID(), nullable=False))
    op.add_column('memories', sa.Column('title', sa.String(length=500), nullable=True))
    op.add_column('memories', sa.Column('content_hash', sa.String(length=64), nullable=False))
    op.create_index(op.f('ix_memories_content_hash'), 'memories', ['content_hash'], unique=False)
    op.create_index(op.f('ix_memories_user_id'), 'memories', ['user_id'], unique=False)
    op.create_foreign_key(None, 'memories', 'users', ['user_id'], ['id'], ondelete='CASCADE')
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_constraint(None, 'memories', type_='foreignkey')
    op.drop_index(op.f('ix_memories_user_id'), table_name='memories')
    op.drop_index(op.f('ix_memories_content_hash'), table_name='memories')
    op.drop_column('memories', 'content_hash')
    op.drop_column('memories', 'title')
    op.drop_column('memories', 'user_id')
    # ### end Alembic commands ###
```

## migrations/versions/694a99143351_revert_memory_model_to_match_original_.py

```python
"""Revert Memory model to match original ERD

Revision ID: 694a99143351
Revises: 618c51757f22
Create Date: 2025-10-01 18:24:01.162958

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = '694a99143351'
down_revision: Union[str, Sequence[str], None] = '618c51757f22'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f('ix_memories_content_hash'), table_name='memories')
    op.drop_index(op.f('ix_memories_user_id'), table_name='memories')
    op.drop_constraint(op.f('memories_user_id_fkey'), 'memories', type_='foreignkey')
    op.drop_column('memories', 'content_hash')
    op.drop_column('memories', 'title')
    op.drop_column('memories', 'user_id')
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('memories', sa.Column('user_id', sa.UUID(), autoincrement=False, nullable=False))
    op.add_column('memories', sa.Column('title', sa.VARCHAR(length=500), autoincrement=False, nullable=True))
    op.add_column('memories', sa.Column('content_hash', sa.VARCHAR(length=64), autoincrement=False, nullable=False))
    op.create_foreign_key(op.f('memories_user_id_fkey'), 'memories', 'users', ['user_id'], ['id'], ondelete='CASCADE')
    op.create_index(op.f('ix_memories_user_id'), 'memories', ['user_id'], unique=False)
    op.create_index(op.f('ix_memories_content_hash'), 'memories', ['content_hash'], unique=False)
    # ### end Alembic commands ###
```

## migrations/versions/add_pgvector_support.py

```python
"""Enable pgvector extension and update embeddings column

Revision ID: add_pgvector_support
Revises: 694a99143351
Create Date: 2025-10-02 10:00:00.000000

"""
from alembic import op
import sqlalchemy as sa
from pgvector.sqlalchemy import Vector

# revision identifiers, used by Alembic.
revision = 'add_pgvector_support'
down_revision = '694a99143351'  # Replace vá»›i revision ID má»›i nháº¥t
branch_labels = None
depends_on = None

def upgrade():
    # Enable pgvector extension
    op.execute('CREATE EXTENSION IF NOT EXISTS vector')
    
    # Drop old embedding column if exists
    op.drop_column('memories', 'embedding')
    
    # Add new pgvector embedding column (768 dimensions for text-embedding-004)
    op.add_column('memories', sa.Column('embedding', Vector(768), nullable=True))
    
    # Create index for fast similarity search
    # "CREATE INDEX CONCURRENTLY" cannot be run inside a transaction block.
    # Use Alembic's autocommit_block to run it outside the transaction so Postgres accepts it.
    with op.get_context().autocommit_block():
        op.execute(
            'CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_memories_embedding '
            'ON memories USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100)'
        )

def downgrade():
    # Drop pgvector index
    op.execute('DROP INDEX IF EXISTS idx_memories_embedding')
    
    # Drop pgvector column
    op.drop_column('memories', 'embedding')
    
    # Add back old embedding column
    op.add_column('memories', sa.Column('embedding', sa.ARRAY(sa.Float), nullable=True))
    
    # Drop pgvector extension (careful - only if no other tables use it)
    # op.execute('DROP EXTENSION IF EXISTS vector')
```

## migrations/versions/add_project_fields_20251013.py

```python
"""add project fields for UI

Revision ID: add_project_fields_20251013
Revises: merge_20251013_01
Create Date: 2025-10-13 00:00:00.000000

Add columns used by the UI: is_active, repo_url, technologies (JSONB),
memories_count, members_count, last_active_at. Backfill counts from
existing tables.
"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql
from sqlalchemy import text

# revision identifiers, used by Alembic.
revision = 'add_project_fields_20251013'
down_revision = 'merge_20251013_01'
branch_labels = None
depends_on = None


def upgrade():
    # Add columns
    op.add_column('projects', sa.Column('is_active', sa.Boolean(), server_default=sa.text('true'), nullable=False))
    op.add_column('projects', sa.Column('repo_url', sa.String(), nullable=True))
    op.add_column('projects', sa.Column('technologies', postgresql.JSONB(), nullable=True))
    op.add_column('projects', sa.Column('memories_count', sa.Integer(), server_default='0', nullable=False))
    op.add_column('projects', sa.Column('members_count', sa.Integer(), server_default='0', nullable=False))
    op.add_column('projects', sa.Column('last_active_at', sa.DateTime(timezone=True), nullable=True))

    # Backfill aggregated counts and last_active_at from related tables
    conn = op.get_bind()
    # Backfill memories_count
    conn.execute(text(
        """
        UPDATE projects
        SET memories_count = sub.cnt
        FROM (SELECT project_id, COUNT(*) AS cnt FROM memories GROUP BY project_id) AS sub
        WHERE projects.id = sub.project_id
        """
    ))

    # Backfill members_count
    conn.execute(text(
        """
        UPDATE projects
        SET members_count = sub.cnt
        FROM (SELECT project_id, COUNT(*) AS cnt FROM user_projects GROUP BY project_id) AS sub
        WHERE projects.id = sub.project_id
        """
    ))

    # Backfill last_active_at from search_logs (if present)
    conn.execute(text(
        """
        UPDATE projects
        SET last_active_at = sub.max_dt
        FROM (SELECT project_id, MAX(created_at) AS max_dt FROM search_logs GROUP BY project_id) AS sub
        WHERE projects.id = sub.project_id
        """
    ))


def downgrade():
    # Remove added columns (reverse of upgrade)
    op.drop_column('projects', 'last_active_at')
    op.drop_column('projects', 'members_count')
    op.drop_column('projects', 'memories_count')
    op.drop_column('projects', 'technologies')
    op.drop_column('projects', 'repo_url')
    op.drop_column('projects', 'is_active')
```

## migrations/versions/add_supabase_user_id_20251009.py

```python
"""add supabase_user_id to users

Revision ID: add_supabase_user_id_20251009
Revises: 618c51757f22
Create Date: 2025-10-09 00:00:00.000000

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa


# revision identifiers, used by Alembic.
revision: str = 'add_supabase_user_id_20251009'
down_revision: Union[str, Sequence[str], None] = '618c51757f22'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('users', sa.Column('supabase_user_id', sa.String(), nullable=True))
    op.create_index(op.f('ix_users_supabase_user_id'), 'users', ['supabase_user_id'], unique=False)
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f('ix_users_supabase_user_id'), table_name='users')
    op.drop_column('users', 'supabase_user_id')
    # ### end Alembic commands ###
```

## migrations/versions/drop_projects_name_unique_20251013.py

```python
"""drop unique constraint on projects.name

Revision ID: drop_projects_name_unique_20251013
Revises: merge_20251013_02
Create Date: 2025-10-13 01:15:00.000000

This migration removes the unique constraint on projects.name so duplicate
project names are allowed (we use repo_url for uniqueness when present).
"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa

# revision identifiers, used by Alembic.
revision: str = 'drop_name_unique_20251013'
down_revision: Union[str, Sequence[str], None] = 'merge_20251013_02'
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    conn = op.get_bind()
    # Drop the unique constraint created by the initial migration. Constraint name
    # observed in error logs: projects_name_key. If it differs, user should adjust.
    with op.batch_alter_table('projects') as batch_op:
        try:
            batch_op.drop_constraint('projects_name_key', type_='unique')
        except Exception:
            # Fallback: try common constraint name
            try:
                batch_op.drop_constraint('projects_name_key', type_='unique')
            except Exception:
                # If constraint not found, continue silently
                pass


def downgrade() -> None:
    with op.batch_alter_table('projects') as batch_op:
        batch_op.create_unique_constraint('projects_name_key', ['name'])
```

## migrations/versions/merge_20251013_02.py

```python
"""merge add_project_fields_20251013 and remove_apikey_from_project

Revision ID: merge_20251013_02
Revises: add_project_fields_20251013, remove_apikey_from_project
Create Date: 2025-10-13 00:30:00.000000

This is a merge revision created to unify two heads in the migration history.
"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa

# revision identifiers, used by Alembic.
revision: str = 'merge_20251013_02'
down_revision: Union[str, Sequence[str], None] = ('add_project_fields_20251013', 'remove_apikey_from_project')
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # Merge revision - no schema changes. This file exists only to unify heads.
    pass


def downgrade() -> None:
    # Downgrade intentionally left empty; reverting a merge is non-trivial.
    pass
```

## migrations/versions/merge_9f1b2c3d4e5f_add_pgvector_support.py

```python
"""merge heads 9f1b2c3d4e5f and add_pgvector_support

Revision ID: merge_9f1b2c3d4e5f_addpg
Revises: 9f1b2c3d4e5f, add_pgvector_support
Create Date: 2025-10-07 00:30:00.000000

This is a merge revision created to unify two heads in the migration history.
"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa

# revision identifiers, used by Alembic.
revision: str = 'merge_9f1b2c3d4e5f_addpg'
down_revision: Union[str, Sequence[str], None] = ('9f1b2c3d4e5f', 'add_pgvector_support')
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # Merge revision - no schema changes. This file exists only to unify heads.
    pass


def downgrade() -> None:
    # Downgrade intentionally left empty; reverting a merge is non-trivial.
    pass
```

## migrations/versions/merge_addsupabase_and_merge9f_20251013.py

```python
"""merge add_supabase_user_id_20251009 and merge_9f1b2c3d4e5f_addpg

Revision ID: merge_addsupabase_and_merge9f_20251013
Revises: add_supabase_user_id_20251009, merge_9f1b2c3d4e5f_addpg
Create Date: 2025-10-13 00:00:00.000000

This is a merge revision created to unify two heads in the migration history.
"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa

# revision identifiers, used by Alembic.
revision: str = 'merge_20251013_01'
down_revision: Union[str, Sequence[str], None] = ('add_supabase_user_id_20251009', 'merge_9f1b2c3d4e5f_addpg')
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # Merge revision - no schema changes. This file exists only to unify heads.
    pass


def downgrade() -> None:
    # Downgrade intentionally left empty; reverting a merge is non-trivial.
    pass
```

## README.md

````markdown
# test-project



## Getting started

To make it easy for you to get started with GitLab, here's a list of recommended next steps.

Already a pro? Just edit this README.md and make it your own. Want to make it easy? [Use the template at the bottom](#editing-this-readme)!

## Add your files

- [ ] [Create](https://docs.gitlab.com/ee/user/project/repository/web_editor.html#create-a-file) or [upload](https://docs.gitlab.com/ee/user/project/repository/web_editor.html#upload-a-file) files
- [ ] [Add files using the command line](https://docs.gitlab.com/topics/git/add_files/#add-files-to-a-git-repository) or push an existing Git repository with the following command:

```
cd existing_repo
git remote add origin https://gitlab.rockitflow.com/truongtienanh/test-project.git
git branch -M main
git push -uf origin main
```

## Integrate with your tools

- [ ] [Set up project integrations](https://gitlab.rockitflow.com/truongtienanh/test-project/-/settings/integrations)

## Collaborate with your team

- [ ] [Invite team members and collaborators](https://docs.gitlab.com/ee/user/project/members/)
- [ ] [Create a new merge request](https://docs.gitlab.com/ee/user/project/merge_requests/creating_merge_requests.html)
- [ ] [Automatically close issues from merge requests](https://docs.gitlab.com/ee/user/project/issues/managing_issues.html#closing-issues-automatically)
- [ ] [Enable merge request approvals](https://docs.gitlab.com/ee/user/project/merge_requests/approvals/)
- [ ] [Set auto-merge](https://docs.gitlab.com/user/project/merge_requests/auto_merge/)

## Test and Deploy

Use the built-in continuous integration in GitLab.

- [ ] [Get started with GitLab CI/CD](https://docs.gitlab.com/ee/ci/quick_start/)
- [ ] [Analyze your code for known vulnerabilities with Static Application Security Testing (SAST)](https://docs.gitlab.com/ee/user/application_security/sast/)
- [ ] [Deploy to Kubernetes, Amazon EC2, or Amazon ECS using Auto Deploy](https://docs.gitlab.com/ee/topics/autodevops/requirements.html)
- [ ] [Use pull-based deployments for improved Kubernetes management](https://docs.gitlab.com/ee/user/clusters/agent/)
- [ ] [Set up protected environments](https://docs.gitlab.com/ee/ci/environments/protected_environments.html)

***

# Editing this README

When you're ready to make this README your own, just edit this file and use the handy template below (or feel free to structure it however you want - this is just a starting point!). Thanks to [makeareadme.com](https://www.makeareadme.com/) for this template.

## Suggestions for a good README

Every project is different, so consider which of these sections apply to yours. The sections used in the template are suggestions for most open source projects. Also keep in mind that while a README can be too long and detailed, too long is better than too short. If you think your README is too long, consider utilizing another form of documentation rather than cutting out information.

## Name
Choose a self-explaining name for your project.

## Description
Let people know what your project can do specifically. Provide context and add a link to any reference visitors might be unfamiliar with. A list of Features or a Background subsection can also be added here. If there are alternatives to your project, this is a good place to list differentiating factors.

## Badges
On some READMEs, you may see small images that convey metadata, such as whether or not all the tests are passing for the project. You can use Shields to add some to your README. Many services also have instructions for adding a badge.

## Visuals
Depending on what you are making, it can be a good idea to include screenshots or even a video (you'll frequently see GIFs rather than actual videos). Tools like ttygif can help, but check out Asciinema for a more sophisticated method.

## Installation
Within a particular ecosystem, there may be a common way of installing things, such as using Yarn, NuGet, or Homebrew. However, consider the possibility that whoever is reading your README is a novice and would like more guidance. Listing specific steps helps remove ambiguity and gets people to using your project as quickly as possible. If it only runs in a specific context like a particular programming language version or operating system or has dependencies that have to be installed manually, also add a Requirements subsection.

## Usage
Use examples liberally, and show the expected output if you can. It's helpful to have inline the smallest example of usage that you can demonstrate, while providing links to more sophisticated examples if they are too long to reasonably include in the README.

## Support
Tell people where they can go to for help. It can be any combination of an issue tracker, a chat room, an email address, etc.

## Roadmap
If you have ideas for releases in the future, it is a good idea to list them in the README.

## Contributing
State if you are open to contributions and what your requirements are for accepting them.

For people who want to make changes to your project, it's helpful to have some documentation on how to get started. Perhaps there is a script that they should run or some environment variables that they need to set. Make these steps explicit. These instructions could also be useful to your future self.

You can also document commands to lint the code or run tests. These steps help to ensure high code quality and reduce the likelihood that the changes inadvertently break something. Having instructions for running tests is especially helpful if it requires external setup, such as starting a Selenium server for testing in a browser.

## Authors and acknowledgment
Show your appreciation to those who have contributed to the project.

## License
For open source projects, say how it is licensed.

## Project status
If you have run out of energy or time for your project, put a note at the top of the README saying that development has slowed down or stopped completely. Someone may choose to fork your project or volunteer to step in as a maintainer or owner, allowing your project to keep going. You can also make an explicit request for maintainers.
````

## tests/test.ipynb

```text
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "657e93e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bar\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Database Connection Test Script\n",
    "Test PostgreSQL and Redis connections independently\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path(__file__).parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from app.db.connect_db import test_connections, initialize_all_databases, db_manager\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main test function\"\"\"\n",
    "    print(\"CodeMemory Database Connection Test\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check environment variables\n",
    "    print(\"\\n Environment Check:\")\n",
    "    required_vars = [\"DATABASE_URL\", \"GOOGLE_API_KEY\"]\n",
    "    optional_vars = [\"REDIS_URL\", \"REDIS_HOST\", \"SUPABASE_URL\"]\n",
    "    \n",
    "    for var in required_vars:\n",
    "        value = os.getenv(var)\n",
    "        status = \"Set\" if value else \"Missing\"\n",
    "        print(f\"  {var}: {status}\")\n",
    "    \n",
    "    for var in optional_vars:\n",
    "        value = os.getenv(var)\n",
    "        status = \"Set\" if value else \"Not set\"\n",
    "        print(f\"  {var}: {status}\")\n",
    "    \n",
    "    print(\"\\n Initializing connections...\")\n",
    "    \n",
    "    # Initialize and test connections\n",
    "    success = await initialize_all_databases()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n Running detailed tests...\")\n",
    "        test_success = await test_connections()\n",
    "        \n",
    "        if test_success:\n",
    "            print(\"\\n All tests passed! Database connections are working properly.\")\n",
    "        else:\n",
    "            print(\"\\n Some tests failed. Check your configuration.\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"\\n Failed to initialize databases. Check your configuration.\")\n",
    "        return False\n",
    "    \n",
    "    # Cleanup\n",
    "    await db_manager.close_connections()\n",
    "    return True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        success = asyncio.run(main())\n",
    "        sys.exit(0 if success else 1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n Test interrupted by user\")\n",
    "        sys.exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n Test failed with error: {e}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4af0a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3723520d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "min_ds-env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
```

## Statistics

- Total Files: 27
- Total Characters: 138773
- Total Tokens: 0
